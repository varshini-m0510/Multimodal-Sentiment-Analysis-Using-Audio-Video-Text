# -*- coding: utf-8 -*-
"""Hackathon-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Me5xARcwv_ndDemspBN30fjpL3wdSHlN
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from transformers import BertTokenizer, BertModel
import torch
import librosa
import cv2
from tqdm import tqdm

from google.colab import drive
drive.mount('/content/drive')

import zipfile
import os
# Step 2: Define the path to the zip file and the destination folder/content/Dataset/Set3_Sentiment/train/train_prop_sent_data3.zip
zip_path = '/content/drive/MyDrive/Dataset.zip'
extract_folder = '/content/Dataset'  # Destination folder in Colab's file system

# Step 3: Create the destination folder if it doesn't exist
os.makedirs(extract_folder, exist_ok=True)

# Step 4: Extract the zip file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_folder)

print("Extraction complete. Files are available in:", extract_folder)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from transformers import BertTokenizer, BertModel
import torch
import librosa
import cv2
from tqdm import tqdm

# Load Data
train_df = pd.read_csv('/content/Dataset/Set3_Sentiment/train/train_prop_sent_csv3_final.csv', encoding='ISO-8859-1')
test_df = pd.read_csv('/content/Dataset/Set3_Sentiment/test/test_prop_sent_csv3_final.csv', encoding='ISO-8859-1')

# Path to video files
video_dir = '/content/Dataset/Set3_Sentiment/train/train_prop_sent_data'
# test_video_dir = '/content/Dataset/Set3_Sentiment/test/test_prop_sent_data'
# Function to get video file path from IDs
def get_video_clip_path(row):
    dialogue_id = row['Dialogue_ID']
    utterance_id = row['Utterance_ID']
    filename = f"dia{dialogue_id}_utt{utterance_id}.mp4"
    return os.path.join(video_dir, filename)

# Apply the function to get file paths for each sampled clip
train_df['video_clip_path'] = train_df.apply(get_video_clip_path, axis=1)
# test_df['video_clip_path'] = test_df.apply(get_video_clip_path, axis=1)
train_df.head()

video_dir = '/content/Dataset/Set3_Sentiment/test/test_prop_sent_data'
# Function to get video file path from IDs
def get_video_clip_path(row):
    dialogue_id = row['Dialogue_ID']
    utterance_id = row['Utterance_ID']
    filename = f"dia{dialogue_id}_utt{utterance_id}.mp4"
    return os.path.join(video_dir, filename)

# Apply the function to get file paths for each sampled clip
test_df['video_clip_path'] = test_df.apply(get_video_clip_path, axis=1)
# test_df['video_clip_path'] = test_df.apply(get_video_clip_path, axis=1)
test_df.head()

# Load BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

def get_text_embeddings(text):
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    # Take the mean of the last hidden state to get sentence embedding
    sentence_embedding = torch.mean(outputs.last_hidden_state, dim=1)
    return sentence_embedding

# Apply BERT to each text entry in the dataset
train_df['text_embeddings'] = train_df['Utterance'].apply(lambda x: get_text_embeddings(x).numpy())

test_df['text_embeddings'] = test_df['Utterance'].apply(lambda x: get_text_embeddings(x).numpy())

test_df.head()

import librosa

# Function to extract MFCC features from video audio
def extract_audio_features(video_path, sr=22050, n_mfcc=13):
    try:
        # Load the audio from video file using librosa
        audio, sample_rate = librosa.load(video_path, sr=sr)
        # Extract MFCC features
        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)
        # Take the mean across the time axis to get a fixed-length feature vector
        mfccs_mean = np.mean(mfccs, axis=1)
        return mfccs_mean
    except Exception as e:
        print(f"Error processing {video_path}: {e}")
        return np.zeros(n_mfcc)  # Return a zero vector if there's an error

# Apply the audio feature extraction function to each video clip path
tqdm.pandas()  # Progress bar for apply
train_df['audio_features'] = train_df['video_clip_path'].progress_apply(lambda x: extract_audio_features(x))

test_df['audio_features'] = test_df['video_clip_path'].progress_apply(lambda x: extract_audio_features(x))

import cv2
from torchvision import models, transforms
import torch

# Load a pre-trained ResNet model
resnet = models.resnet50(pretrained=True)
resnet = resnet.eval()  # Set to evaluation mode

# Define transformation for input frames
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((224, 224)),  # ResNet requires 224x224 input
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

def extract_video_features(video_path, model=resnet, num_frames=5):
    features = []
    try:
        cap = cv2.VideoCapture(video_path)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        frame_indices = np.linspace(0, frame_count - 1, num_frames, dtype=int)

        for idx in frame_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
            ret, frame = cap.read()
            if not ret:
                continue
            # Apply transformation and add batch dimension
            input_tensor = transform(frame).unsqueeze(0)
            with torch.no_grad():
                frame_features = model(input_tensor)
            # Take the mean across the channels for a single feature vector per frame
            features.append(frame_features.squeeze().numpy())

        cap.release()

        # Average frame features to get a single feature vector for the video
        if features:
            return np.mean(features, axis=0)
        else:
            return np.zeros((2048,))  # Return zero vector if no frames processed
    except Exception as e:
        print(f"Error processing {video_path}: {e}")
        return np.zeros((2048,))

# Apply the video feature extraction function
train_df['video_features'] = train_df['video_clip_path'].progress_apply(lambda x: extract_video_features(x))

test_df['video_features'] = test_df['video_clip_path'].progress_apply(lambda x: extract_video_features(x))

train_df.fillna(0, inplace=True)
test_df.fillna(0, inplace=True)

import pandas as pd

# Assuming your DataFrame is named 'df'
train_df['Sentiment_Encoded'] = train_df['Sentiment'].map({'negative': 0, 'neutral': 1, 'positive': 2})

# Display the updated DataFrame
print(train_df[['Sentiment', 'Sentiment_Encoded']])

from sklearn.model_selection import train_test_split

# Split the dataset into training and testing sets (80% training, 20% testing)
train_set, test_set = train_test_split(train_df, test_size=0.2, random_state=42)

# Check the size of the splits
print(f"Training Set Size: {train_set.shape}")
print(f"Testing Set Size: {test_set.shape}")


import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np

# Automatically detect input sizes based on the first sample in the dataset
text_input_size = np.array(train_set['text_embeddings'].tolist()).shape[2]
audio_input_size = np.array(train_set['audio_features'].tolist()).shape[1]
video_input_size = np.array(train_set['video_features'].tolist()).shape[1]

# Define the multimodal model class
class MultimodalModel(nn.Module):
    def __init__(self, num_classes=3, text_input_size=text_input_size, audio_input_size=audio_input_size, video_input_size=video_input_size):
        super(MultimodalModel, self).__init__()

        # Fully connected layers for each modality
        self.text_fc = nn.Linear(text_input_size, 128)
        self.audio_fc = nn.Linear(audio_input_size, 128)
        self.video_fc = nn.Linear(video_input_size, 128)

        # Final fully connected layer for classification
        self.fc = nn.Linear(128 * 3, num_classes)

    def forward(self, text_input, audio_input, video_input):
        # Flatten all inputs to 2D: (batch_size, feature_size)
        text_input = text_input.view(text_input.size(0), -1)  # Flatten text
        audio_input = audio_input.view(audio_input.size(0), -1)  # Flatten audio
        video_input = video_input.view(video_input.size(0), -1)  # Flatten video

        # Process each modality
        text_output = self.text_fc(text_input)
        audio_output = self.audio_fc(audio_input)
        video_output = self.video_fc(video_input)

        # Concatenate text, audio, and video features
        combined = torch.cat((text_output, audio_output, video_output), dim=1)

        # Final classification layer
        output = self.fc(combined)

        return output

# Dataset class to handle text, audio, and video features along with labels
class MultimodalDataset(Dataset):
    def __init__(self, text_features, audio_features, video_features, labels=None):
        self.text_features = text_features
        self.audio_features = audio_features
        self.video_features = video_features
        self.labels = labels

    def __len__(self):
        return len(self.text_features)

    def __getitem__(self, idx):
        text = self.text_features[idx]
        audio = self.audio_features[idx]
        video = self.video_features[idx]
        if self.labels is not None:
            label = self.labels[idx]
            return text, audio, video, label
        else:
            return text, audio, video

# Convert features to NumPy arrays
X_train_text = np.array(train_set['text_embeddings'].tolist())
X_train_audio = np.array(train_set['audio_features'].tolist())
X_train_video = np.array(train_set['video_features'].tolist())
y_train = np.array(train_set['Sentiment_Encoded'])

# Create Dataset and DataLoader
train_dataset = MultimodalDataset(X_train_text, X_train_audio, X_train_video, y_train)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# Initialize the model
model = MultimodalModel(num_classes=3)

# Loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

epochs = 20
for epoch in range(epochs):
    model.train()
    running_loss = 0.0
    correct_predictions = 0
    total_predictions = 0

    for text, audio, video, labels in train_loader:
        # Move tensors to device and convert to float
        text = text.to(device).float()
        audio = audio.to(device).float()
        video = video.to(device).float()
        labels = labels.to(device)

        optimizer.zero_grad()

        # Forward pass
        outputs = model(text, audio, video)

        # Compute the loss
        loss = criterion(outputs, labels)
        running_loss += loss.item()

        # Compute accuracy
        _, predicted = torch.max(outputs, 1)
        correct_predictions += (predicted == labels).sum().item()
        total_predictions += labels.size(0)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

    # Compute epoch statistics
    avg_loss = running_loss / len(train_loader)
    accuracy = (correct_predictions / total_predictions) * 100

    print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%")

X_test_text = np.array(test_df['text_embeddings'].tolist())
X_test_audio = np.array(test_df['audio_features'].tolist())
X_test_video = np.array(test_df['video_features'].tolist())
# Create Dataset and DataLoader for the test set
test_dataset = MultimodalDataset(X_test_text, X_test_audio, X_test_video, labels=None)  # No labels
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Set the model to evaluation mode
model.eval()  # Disable dropout and batch normalization

predictions = []

# No gradient computation needed for inference
with torch.no_grad():
    for data in test_loader:  # Iterate over batches
        text, audio, video = data  # Unpack only the 3 features (no labels)

        # Move inputs to device
        text = text.to(device).float()
        audio = audio.to(device).float()
        video = video.to(device).float()

        # Forward pass
        outputs = model(text, audio, video)

        # Get predicted class labels (most likely class)
        _, predicted = torch.max(outputs, 1)
        predictions.extend(predicted.cpu().numpy())

# Now, 'predictions' contains the predicted labels for the test set
print(f"Predictions: {predictions}")

print(len(X_test_text))  # Should be equal to len(test_df)
print(len(X_test_audio))  # Should be equal to len(test_df)
print(len(X_test_video))  # Should be equal to len(test_df)

decoded_preds = ["negative" if pred == 0 else "neutral" if pred == 1 else "positive" for pred in predictions]

# Save predictions to CSV
submission_df = pd.DataFrame({
    'Sr No.': test_df["Sr No."],
    'Emotion': decoded_preds
})

submission_df.to_csv("Submission.csv", index=False)
print("Predictions saved to Submission.csv")